{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0134bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup \n",
    "from collections import defaultdict\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea1dd23",
   "metadata": {},
   "source": [
    "Scroll the page to load all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f5e6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The data required for crawling is saved in a dynamic table that updates with more rows everytime the page is\n",
    "scrolled to the bottom, this script will scroll to the bottom of the page revealing all data.\n",
    "if recent is true then the script will scroll to the most recent fire in the saved csv file and stop.\n",
    "'''\n",
    "\n",
    "def scrollPage(URL, recent = False):\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    driver.get(URL)\n",
    "    \n",
    "    try:\n",
    "        # Wait for table to load (max 2 min)\n",
    "        elem = WebDriverWait(driver, 120).until(EC.presence_of_element_located((By.TAG_NAME, \"table\")))\n",
    "        \n",
    "    finally:\n",
    "        last_height = driver.execute_script('return document.querySelector(\"table\").scrollHeight')\n",
    "        element = driver.find_element(By.TAG_NAME, \"table\")\n",
    "        \n",
    "        # FireDiscoveryDateTime\n",
    "        if recent:\n",
    "            df = pd.read_csv(\"Full_Wildland_Fires.csv\")\n",
    "            identifier = df[\"UniqueFireIdentifier\"][0]\n",
    "            \n",
    "            driver.find_element_by_css_selector('#ember91-title').click()\n",
    "            time.sleep(5)\n",
    "            driver.find_element_by_css_selector('#ember91-title').click()\n",
    "            time.sleep(5)\n",
    "        \n",
    "        while True:\n",
    "            element.send_keys(Keys.END)      # Scroll to end of available page\n",
    "            time.sleep(5)                    # Wait for new content to load\n",
    "            \n",
    "            if recent:\n",
    "                if identifier in driver.page_source:\n",
    "                    break\n",
    "                \n",
    "            new_height = driver.execute_script('return document.querySelector(\"table\").scrollHeight')\n",
    "            if new_height == last_height:    # End of page\n",
    "                 break\n",
    "            last_height = new_height\n",
    "\n",
    "        return driver.page_source            # html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b289d7",
   "metadata": {},
   "source": [
    "Crawl the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35894e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "After all the table is loaded, this script will crawl the page and retrieve the data.\n",
    "'''\n",
    "\n",
    "def crawlHTML(htmlText):    \n",
    "    soup = BeautifulSoup(htmlText, \"html.parser\")\n",
    "    titles, columns = {}, defaultdict(list)\n",
    "    i = 0\n",
    "    \n",
    "    # Save the collumn number of each header title for later access to its values\n",
    "    for t in soup.findAll(\"th\"):         \n",
    "        titles[t.find(\"span\").string.strip()] = i\n",
    "        i += 1\n",
    "    \n",
    "    # Create a dictionary of {header title : list of collumn values}\n",
    "    for title in titles:\n",
    "        for value in soup.findAll(\"td\", attrs={\"data-col\":str(titles[title])}):\n",
    "            columns[title].append(value.text)\n",
    "            \n",
    "    return pd.DataFrame(columns)         # DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f92a4f0",
   "metadata": {},
   "source": [
    "Save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451a8777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveDataFrameToCSV(df, path):\n",
    "    df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04dc987",
   "metadata": {},
   "source": [
    "Run Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828abda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runProgram(URL, path_csv, recent = False):\n",
    "    htmlText = scrollPage(URL, recent)\n",
    "    df = crawlHTML(htmlText)\n",
    "    df.columns = df.columns.str.replace(' ', '')    # Remove spaces in titles\n",
    "    saveDataFrameToCSV(df, path_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f35c37",
   "metadata": {},
   "source": [
    "#### Driver Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa940bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "source:         https://data-nifc.opendata.arcgis.com\n",
    "\n",
    "URL_Live:       Page with all ongoing wildfires, USA only, short table\n",
    "URL_History:    Page with all wildfire records since 2014, USA only\n",
    "\n",
    "recent:         True:  use with 'URL_History' to crawl new data that is not in the existing dataset\n",
    "                False: use with 'URL_live' to crawl ongoing fires\n",
    "                \n",
    "Live dataset name:      'Live_Wildland_Fires.csv'\n",
    "Existing dataset name:  'Full_Wildland_Fires.csv'\n",
    "Recent dataset name:    'Recent_Wildland_Fires.csv'\n",
    "\n",
    "-- Change 'path_csv' name and 'recent' value accordingly.\n",
    "'''\n",
    "\n",
    "URL_live = \"https://data-nifc.opendata.arcgis.com/datasets/wfigs-current-wildland-fire-locations/explore?showTable=true\"\n",
    "URL_History = \"https://data-nifc.opendata.arcgis.com/datasets/wfigs-wildland-fire-locations-full-history/explore?showTable=true\"\n",
    "path_csv = \"Recent_Wildland_Fires.csv\"\n",
    "\n",
    "recent = True    \n",
    "\n",
    "runProgram(URL_History, path_csv, recent)\n",
    "\n",
    "print(\"Task complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
