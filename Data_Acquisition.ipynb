{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e0134bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup \n",
    "from collections import defaultdict\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02f5e6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The data required for crawling is saved in a dynamic table that updates with more rows everytime the page is\n",
    "scrolled to the bottom, this script will scroll to the bottom of the page revealing all data.\n",
    "'''\n",
    "\n",
    "def scrollPage(URL):\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    driver.get(URL)\n",
    "    \n",
    "    try:\n",
    "        # Wait for table to load (max 2 min)\n",
    "        elem = WebDriverWait(driver, 120).until(EC.presence_of_element_located((By.TAG_NAME, \"table\")))\n",
    "        \n",
    "    finally:\n",
    "        last_height = driver.execute_script('return document.querySelector(\"table\").scrollHeight')\n",
    "        element = driver.find_element(By.TAG_NAME, \"table\")\n",
    "\n",
    "        while True:\n",
    "            element.send_keys(Keys.END)      # Scroll to end of available page\n",
    "            time.sleep(3)                    # Wait for new content to load\n",
    "\n",
    "            new_height = driver.execute_script('return document.querySelector(\"table\").scrollHeight')\n",
    "            if new_height == last_height:    # End of page\n",
    "                 break\n",
    "            last_height = new_height\n",
    "\n",
    "        return driver.page_source            # html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f35894e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "After all the table is loaded, this script will crawl the page and retrieve the data.\n",
    "'''\n",
    "\n",
    "def crawlHTML(htmlText):    \n",
    "    soup = BeautifulSoup(htmlText, \"html.parser\")\n",
    "    titles, columns = {}, defaultdict(list)\n",
    "    i = 0\n",
    "    \n",
    "    # Save the collumn number of each header title for later access to its values\n",
    "    for t in soup.findAll(\"th\"):         \n",
    "        titles[t.find(\"span\").string.strip()] = i\n",
    "        i += 1\n",
    "    \n",
    "    # Create a dictionary of {header title : list of collumn values}\n",
    "    for title in titles:\n",
    "        for value in soup.findAll(\"td\", attrs={\"data-col\":str(titles[title])}):\n",
    "            columns[title].append(value.text)\n",
    "            \n",
    "    return pd.DataFrame(columns)         # DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "451a8777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveDataFrameToCSV(df, path):\n",
    "    df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "828abda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runProgram(URL, path_csv):\n",
    "    htmlText = scrollPage(URL)\n",
    "    df = crawlHTML(htmlText)\n",
    "    df.columns = df.columns.str.replace(' ', '')    # Remove spaces in titles\n",
    "    saveDataFrameToCSV(df, path_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa940bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDriver Code\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Driver Code\n",
    "'''\n",
    "\n",
    "# URL_History:    Page with all wildfire records since 2014, USA only\n",
    "# URL_Live:       Page with all ongoing wildfires, USA only, short table\n",
    "\n",
    "# URL_History = \"https://data-nifc.opendata.arcgis.com/datasets/wfigs-wildland-fire-locations-full-history/explore?showTable=true\"\n",
    "# URL_live = \"https://data-nifc.opendata.arcgis.com/datasets/wfigs-current-wildland-fire-locations/explore?showTable=true\"\n",
    "# path_csv = \"Wildland Fires.csv\"\n",
    "\n",
    "# runProgram(URL_live, path_csv)\n",
    "\n",
    "# print(\"Task complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce498dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
